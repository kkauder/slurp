#__________________________________________________________________________________________________________________
#___________________________________________________________________________________________________DST_PRESUBMIT__
PRESUBMIT:
  #
  #
  #
  params: 
    name: PRESUBMIT

  presubmit:
    db: 'daq'
    query: |-
        select 'daq/filelist'                                           as source  ,
                runnumber                                                          ,
                0                                                       as segment ,
                string_agg( distinct split_part(filename,'/',-1), ',' ) as files   ,
                string_agg( distinct hostname,',')                      as hosts   
        from filelist
        where filename     like '/bbox%' and
              filename not like '%intt%' and
              filename not like '%LL1%'  and
              filename not like '%GL1%'  and
              filename not like '%TPC%'  and 
              filename not like '%TPOT%' and
              filename not like '%mvtx%'
              {run_condition}
              group by runnumber
              order by runnumber
              {limit_condition}
              ;

    action: |-
       ./presubmit_action.sh {query} 

  


#__________________________________________________________________________________________________________________
#_______________________________________________________________________________________________________DST_CALOR__
DST_CALOR:

   #
   # Params defines replacement variables which can be used in the job
   # definition below.  It also defines condor requirements, such as 
   # memory and disk.  The condor parameters $(par) will be replaced by
   # condor at submission time with values determined in matching the
   # input files to the required output through the kaedama rule.
   # 
   params:
     name:       DST_CALOR_auau23                                                        # name of the production series / dataset type
     build:      ana.387                                                                 # software build
     build_name: ana387                                                                  # ... stripped of the period
     dbtag:      2023p003                                                                # database tag
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)                   # naming convention for logfiles.  Condor will substitute the name, build and dbtag
     outbase :   $(name)_$(build)_$(tag)                                                 # naming convention for the output files (run and segment will be applied by user macro)
     script  :   run_caloreco.sh                                                         # name of the user production script
     payload :   /sphenix/u/sphnxpro/slurp/MDC2/submit/rawdata/caloreco/rundir/          # github directory where production scripts, macros, etc... are found
     mem     :   2048MB                                                                  # memory requirement
     disk    :   2GB                                                                     # disk requirement

   # Database query to find input files.  Note that the {run_condition}, {seg_condition} and
   # {limit_condition} are defined in kaedama from command line arguments.  These should be 
   # left in the query.
   input_query: |-
        select filename,runnumber,segment from datasets
            where filename like 'DST_EVENT_auau23_ana393_2023p009-%'
        {run_condition} 
        {seg_condition}
        order by runnumber,segment
        {limit_condition}

   # Declares the directory where input files are found, output files are stored, and
   # log/condor files should be placed.
   filesystem:
     outdir : "/sphenix/lustre01/sphnxpro/slurp/$$([$(run)/100])00"
     logdir : "file:///sphenix/data/data02/sphnxpro/condorlogs/$$([$(run)/100])00"
     condor :        "/sphenix/data/data02/sphnxpro/condorlogs/$$([$(run)/100])00"

   # 
   # Specifies the condor job submission file.  Variables declared in the params
   # and filesystem block may be substituded here using the syntax {variable}.
   # 
   # Condor variables are defined using the syntax $(variable).  
   #
   # Note well -- it is important to ensure that the list of arguments is correctly
   # defined and maps onto the expected input of the payload script.
   #
   job:
     executable             : "{payload}/run_caloreco.sh"
     arguments              : "$(nevents) $(run) $(seg) $(lfn) . $(dst) $(outdir) $(buildarg) $(tag) $(ClusterId) $(ProcId)"
     user_job_wrapper       : "init.sh"
     output_destination     : 'file://{condor}'
     transfer_input_files   : "{payload},cups.py,init.sh,pull.py"
     output                 : "{logbase}.condor.stdout"
     error                  : "{logbase}.condor.stderr"
     log                    : "{condor}/{logbase}.condor"
     accounting_group       : "group_sphenix.mdc2"
     accounting_group_user  : "sphnxpro"
     transfer_output_files  : '$(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d).out,$(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d).err'
#    transfer_output_files  : 'stdout.log,stderr.log'

#__________________________________________________________________________________________________________________
#_______________________________________________________________________________________________________DST_EVENT__
DST_EVENT:
 
   # DST_EVENT works from a pre-built set of run lists.
   params:
     name:       DST_EVENT_auau23
     build:      ana.393
     build_name: ana393
     dbtag:      2023p009
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run.sh
     payload :   /sphenix/u/sphnxpro/slurp/eventcombine/
     mem     :   4096MB   

   #
   # input_query:
   #
   # This builds a list of all runs known to the file catalog "datasets" table.
   # The query should return:
   # 1. The source of the information (formatted as database name/table name)
   # 2. The run number
   # 3. A sequence number (a placeholder fixed at zero for event builders)
   # 4. And a space-separated list of logical filenames 
   #
   # The input query below accesses the datasets table of the file catalog.
   # The dsttype is "beam" and the dataset is rawdata1008.
   #
   # The {*_condition} parameters (run, seg, limit) are substituted by kaedama
   # based on (optional) command line options.
   #   
   input_query: |-
         select 
                'filecatalog/datasets'                                  as source  ,
                runnumber                                                          ,
                0                                                       as segment ,        
                string_agg( distinct split_part(filename,'/',-1), ' ' ) as files
         from 
                datasets
         where
                dsttype='beam' and dataset='rawdata1008'
         {run_condition} 
         {seg_condition}
         group by 
                runnumber
         order by 
                runnumber
         {limit_condition}
         ;

   #
   # runlist_query:
   #
   # This builds a list of all runs known to the daq database "filelist" table. 
   # 
   # The query should return:
   # 1. The source of the information (formatted as database name/table name)
   # 2. The run number
   # 3. A sequence number (a placeholder fixed at zero for event builders)
   # 4. A space-separated list of logical filenames 
   # 5. A space-separated list of daq hosts.
   # 
   # grep -v intt | grep -v LL1 | grep -v GL1 | grep -v TPC | grep -v TPOT | grep -v mvtx

   runlist_query: |-
        select 'daq/filelist'                                           as source  ,
                runnumber                                                          ,
                0                                                       as segment ,
                string_agg( distinct split_part(filename,'/',-1), ' ' ) as files   ,
                string_agg( distinct hostname,' ')                      as hosts   
        from filelist
        where filename     like '/bbox%' and
              filename not like '%intt%' and
              filename not like '%LL1%'  and
              filename not like '%GL1%'  and
              filename not like '%TPC%'  and 
              filename not like '%TPOT%' and
              filename not like '%mvtx%' 
              {run_condition}
              group by runnumber
              order by runnumber
              {limit_condition}
              ;


#             and hostname in ( 'seb00','seb01','seb02','seb03','seb04','seb05','seb06','seb07','seb14','seb15','seb16','seb17','seb18' )
#             and transferred_to_sdcc

   filesystem:
     outdir : "/sphenix/lustre01/sphnxpro/slurp/$$([$(run)/100])00"
     logdir : "file:///sphenix/data/data02/sphnxpro/condorlogs/$$([$(run)/100])00"
     condor :        "/sphenix/data/data02/sphnxpro/condorlogs/$$([$(run)/100])00"

   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     executable            : "{payload}/run.sh"
     user_job_wrapper      : "init.sh"
     arguments             : "$(nevents) {outbase} {logbase} {outdir} $(run) $(ClusterId) $(ProcId) $(build) $(tag) $(inputs)"
     output_destination    : 'file://{condor}'
     transfer_input_files  : "{payload},cups.py,init.sh,pull.py"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"



  