#__________________________________________________________________________________________________________________
#_______________________________________________________________________________________________________DST_CALOR__
DST_CALOR:

   #
   # Params defines replacement variables which can be used in the job
   # definition below.  It also defines condor requirements, such as 
   # memory and disk.  The condor parameters $(par) will be replaced by
   # condor at submission time with values determined in matching the
   # input files to the required output through the kaedama rule.
   # 
   params:
     name:       DST_CALOR_auau23                                                        # name of the production series / dataset type
     build:      ana.387                                                                 # software build
     build_name: ana387                                                                  # ... stripped of the period
     dbtag:      2023p003                                                                # database tag
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)                   # naming convention for logfiles.  Condor will substitute the name, build and dbtag
     outbase :   $(name)_$(build)_$(tag)                                                 # naming convention for the output files (run and segment will be applied by user macro)
     script  :   run_caloreco.sh                                                         # name of the user production script
     payload :   /sphenix/u/sphnxpro/slurp/MDC2/submit/rawdata/caloreco/rundir/          # github directory where production scripts, macros, etc... are found
     mem     :   2048MB                                                                  # memory requirement
     disk    :   2GB                                                                     # disk requirement

   # Database query to find input files.  Note that the {run_condition}, {seg_condition} and
   # {limit_condition} are defined in kaedama from command line arguments.  These should be 
   # left in the query.
   input_query: |-
        select filename,runnumber,segment from datasets
            where filename like 'DST_EVENT_auau23_ana393_2023p009-%'
        {run_condition} 
        {seg_condition}
        order by runnumber,segment
        {limit_condition}

   # Declares the directory where input files are found, output files are stored, and
   # log/condor files should be placed.
   filesystem:
     outdir : "/sphenix/lustre01/sphnxpro/slurp/$$([$(run)/100])00"
     logdir : "file:///sphenix/data/data02/sphnxpro/condorlogs/$$([$(run)/100])00"
     condor :        "/sphenix/data/data02/sphnxpro/condorlogs/$$([$(run)/100])00"

   # 
   # Specifies the condor job submission file.  Variables declared in the params
   # and filesystem block may be substituded here using the syntax {variable}.
   # 
   # Condor variables are defined using the syntax $(variable).  
   #
   # Note well -- it is important to ensure that the list of arguments is correctly
   # defined and maps onto the expected input of the payload script.
   #
   job:
     executable             : "{payload}/run_caloreco.sh"
     arguments              : "$(nevents) $(run) $(seg) $(lfn) . $(dst) $(outdir) $(buildarg) $(tag) $(ClusterId) $(ProcId)"
     user_job_wrapper       : "init.sh"
     output_destination     : 'file://{condor}'
     transfer_input_files   : "{payload},cups.py,init.sh,pull.py"
     output                 : "{logbase}.condor.stdout"
     error                  : "{logbase}.condor.stderr"
     log                    : "{condor}/{logbase}.condor"
     accounting_group       : "group_sphenix.mdc2"
     accounting_group_user  : "sphnxpro"
     transfer_output_files  : '$(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d).out,$(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d).err'
#    transfer_output_files  : 'stdout.log,stderr.log'

#__________________________________________________________________________________________________________________
#_______________________________________________________________________________________________________DST_EVENT__
DST_EVENT:
 
   # DST_EVENT works from a pre-built set of run lists.
   params:
     name:       DST_EVENT_auau23
     build:      ana.393
     build_name: ana393
     dbtag:      2023p009
     logbase :   $(name)_$(build)_$(tag)-$INT(run,%08d)-$INT(seg,%04d)
     outbase :   $(name)_$(build)_$(tag)
     script  :   run.sh
     payload :   /sphenix/u/sphnxpro/slurp/eventcombine/
     mem     :   4096MB   
     file_lists:
        - eventcombine/lists/hcaleast_$(run).list
        - eventcombine/lists/hcalwest_$(run).list
        - eventcombine/lists/ll1_$(run).list
        - eventcombine/lists/mbd_$(run).list
        - eventcombine/lists/seb00_$(run).list
        - eventcombine/lists/seb01_$(run).list
        - eventcombine/lists/seb02_$(run).list
        - eventcombine/lists/seb03_$(run).list
        - eventcombine/lists/seb04_$(run).list
        - eventcombine/lists/seb05_$(run).list
        - eventcombine/lists/seb06_$(run).list
        - eventcombine/lists/seb07_$(run).list
        - eventcombine/lists/zdc_$(run).list

   #
   # This query builds a list of runs where we have one or more input files to
   # the event builder.  Future version of this will be more complicated to 
   # lookup runs that have been completed, transfered, and have all files expected
   # based on the online run configuration.
   #
   input_query: |-
         select 
                'dummy'     as dummy,
                runnumber           ,
                0 as segment        ,
                string_agg( distinct split_part(filename,'/',-1), ' ' ) as files
         from 
                datasets
         where
                dsttype='beam' and dataset='rawdata1008'
         {run_condition} 
         {seg_condition}
         group by 
                runnumber
         order by 
                runnumber
         {limit_condition}
         ;

   runlist_query: |-
        select 'dummy'                                                  as dummy   ,
                runnumber                                                          ,
                0                                                       as segment ,
                string_agg( distinct split_part(filename,'/',-1), ' ' ) as files   ,
                string_agg( distinct hostname,' ')                      as hosts   
        from filelist
        where transferred_to_sdcc
              and filename like '/bbox%'
              and hostname in ( 'seb00','seb01','seb02','seb03','seb04','seb05','seb06','seb07','seb14','seb15','seb16','seb17','seb18' )
              {run_condition}
              group by runnumber
              order by runnumber
              {limit_condition}
              ;


   filesystem:
     outdir : "/sphenix/lustre01/sphnxpro/slurp/$$([$(run)/100])00"
     logdir : "file:///sphenix/data/data02/sphnxpro/condorlogs/$$([$(run)/100])00"
     condor :        "/sphenix/data/data02/sphnxpro/condorlogs/$$([$(run)/100])00"

   #
   # Again I note the need to ensure that the arguments are properly specified given the
   # definition of the payload script.
   #
   job:
     executable            : "{payload}/run.sh"
     user_job_wrapper      : "init.sh"
     arguments             : "$(nevents) {outbase} {logbase} {outdir} $(run) $(ClusterId) $(ProcId) $(build) $(tag)"
     output_destination    : 'file://{condor}'
     transfer_input_files  : "{payload},cups.py,init.sh,pull.py,{file_lists}"
     output                : '{logbase}.condor.stdout'
     error                 : '{logbase}.condor.stderr'
     log                   : '{condor}/{logbase}.condor'
     accounting_group      : "group_sphenix.mdc2"
     accounting_group_user : "sphnxpro"



  